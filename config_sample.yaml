# File contains muster configuration file for credativ-pg-migrator

## Configuration for the pre migration analysis
pre_migration_analysis:
  # Section for listing TOP N tables from the source database
  # by_rows - list tables by number of rows in the table - value is the number of TOP N tables to list
  # part set to 0 is skipped, not listed
  top_n_tables:
    by_rows: 10
    by_size: 10
    by_columns: 10
    by_indexes: 10
    by_constraints: 10

env_variables:
  # List of environment variables to set before running the migration
  # This is useful for setting up libraries that require specific environment variables
  # Example: setting up library paths or locale settings
  - name: "LD_LIBRARY_PATH"
    value: "/usr/local/lib:/usr/lib:/lib"
  - name: "LANG"
    value: "en_US.UTF-8"

# Migrator database connection for metadata storage
# Migrator created multiple tables in the database
# In most cases we presume migrator database is the same as target database, just with different schema
migrator:
  type: "postgresql"
  host: "localhost"
  port: 5432
  username: "postgres"
  password: "postgres"
  database: "database"
  schema: "migration"

# source database connection
# type: "informix", "sybase_ase", "mssql", "ibm_db2", "mysql", "sql_anywhere", "postgresql"
source:
  type: "sybase_ase"
  host: "localhost"
  port: 5000
  username: "sa"
  password: "password"
  database: "source_database"
  # schema or owner - depending on the source database - not both
  schema: "dbo"
  # connectivity type - "jdbc", "odbc", "native"
  # "native" does not have any additional section
  connectivity: "odbc"
  jdbc:
    driver: "com.sybase.jdbc4.jdbc.SybDriver"
    libraries: "../lib/jdbc/jconn4.jar"
  odbc:
    driver: 'FreeTDS'
    libraries: "/usr/lib/x86_64-linux-gnu/odbc/libtdsodbc.so"
  # system catalog is used only for some database
  # ibm_db2 - SYSCAT or SYSIBM - sysibm simulates the information_schema
  # mssql - SYS or INFORMATION_SCHEMA
  system_catalog: "SYSIBM"
  # informix specific setting
  db_locale: "en_US.utf8"  # locale for the Informix database, used for date and time formatting

# target database connection - always postgresql
target:
  type: "postgresql"
  host: "localhost"
  port: 5432
  username: "postgres"
  password: "postgres"
  database: "target_database"
  schema: "target_schema"
  # settings for the migration process
  # are optional, if present they will override the default settings in every connection
  settings:
    work_mem: '32MB'
    maintenance_work_mem: '512MB'
    role: 'target_owner'
    search_path: "target_schema, public"

# recipe for the migration process
migration:
  # drop schema if exists - uses DROP CASCADE if true
  drop_schema: true
  drop_tables: true
  truncate_tables: true
  create_tables: true
  migrate_data: true
  migrate_indexes: true
  migrate_constraints: true
  migrate_funcprocs: true
  migrate_triggers: true
  migrate_views: true
  set_sequnces: true
  on_error: continue # stop, continue
  parallel_workers: 8
  # batch_size: number of rows to migrate in one batch during data migration, default is 100000
  # batch size is used when data are repeatedly read from source database and written to the target database
  batch_size: 10000
  # data_chunk_size: number of rows to migrate in one chunk during data migration, default is 1000000
  # must be bigger than batch_size
  # chunk size is used to divide migration of huge tables into smaller independent parts - chunks are logged and processed separately
  # intention is to allow to pause the migration process and continue later, in case if the source database contains really huge tables
  # or if maintenance window is limited only to some daily hours
  # chunk is processed in batches of batch_size
  data_chunk_size: 1000000
  # PostgreSQL scripts for the migration process
  # Both run on the target database, before and after the migration
  # pre_migration_script: pre_migration.sql
  # post_migration_script: post_migration.sql
  names_case_handling: lower # lower, upper, keep - if names of the objects should be converted to lower or upper case or kept as is
  # if varchar length is bigger than this value, it will be converted to text
  # 0 = always change to text, -1 = never change to text, always migrate as varchar, >0 = convert varchar to text if length is >= this value
  # if parameter is missing, default is -1 -> migrate all varchars as varchars as they are
  varchar_to_text_length: 4000
  char_to_text_length: 10
  # migrate_lob_values - if true, migrate LOB values (BLOB, CLOB) as text or bytea
  # if false, LOB values will be skipped (migrated as NULL)
  # if missing, default is true - LOB values will be migrated
  migrate_lob_values: false
  # automatic scheduled actions for the migration process
  # - list of actions to be performed at specific times during the migration process
  # action can be scheduled based on date and time or based on timer - in hours
  # timer is currently not implemented, so only datetime is used
  # actions:
  # - "pause" - pause the migration process at the specified time,
  # - "stop" - stop the migration process at the specified time,
  # - "continue" - continue the migration process at the specified time, if it was paused
  # if time is not specified, action is ignored
  scheduled_actions:
    - name: "pause at 1:00 in the morning"
      datetime: "2025.07.11 01:00"
      # timer_hours: xx
      action: "pause"  # pause, stop, continue

table_settings:
  # Specify individual table settings for the migration process - there override the global settings for the specific table matching the table_name
  # table_name can be full table name or regular expression
  # available settings:
  # - batch_size: number of rows to migrate in one batch, default is 100000
  # - data_chunk_size: number of rows to migrate in one chunk, default is 1000000
  # - migrate_data: true/false - if data should be migrated, default is true
  # - migrate_indexes: true/false - if indexes should be migrated, default is true
  # - migrate_constraints: true/false - if constraints should be migrated, default is true
  # - migrate_triggers: true/false - if triggers should be migrated, default is true
  - table_name: "table1"
    batch_size: 50000
    data_chunk_size: 1000000
    migrate_data: true
    migrate_indexes: false
    migrate_constraints: true
    migrate_triggers: false

  - table_name: "table2"
    batch_size: 20000

# Specify tables to include in the migration - list of table names or regular expressions
# Empty list means all tables will be included
# One string value added without putting it into list, will raise an parsing error
include_tables:
  # - "table1"
  # - "table_prefix_*"

# Specify tables to exclude from the migration - list of table names or regular expressions
# Empty list means no tables will be excluded
exclude_tables:
  # - "z_skins"
  # - "z_skins_er*"

include_views: all
  # - "view1"
  # - "view_prefix_*"

exclude_views:
  # - "z_skins"
  # - "z_skins_er*"

include_funcprocs: all
  # - "func1"

exclude_funcprocs:
  # - "proc1"


# Data types substitution
# table name, column name, source_data_type (regexp - eventually including length), target_data_type (mandatory, precise value including length), comment
# table_name, column_name and source_data_type are all optional, but at least one of them, most likely column_name or source_data_type, must be specified
# Source data type can be specified as precise match, pattern for LIKE operator or regular expression - checked in this order
# Matching is case-insensitive, regexp is preferred over LIKE
data_types_substitution:
  - ["", "", "TypMacAdresse", "TEXT", 'Does not have direct equivalent in PostgreSQL, using TEXT']
  - ["", "", "TypID", "BIGINT", 'Numeric PK is not supported in PostgreSQL, using BIGINT']
  - ["", "", "numeric(_,0)", "INTEGER", 'Sybase numeric type with no decimal places, using INTEGER']
  - ["", "", "numeric(1_,0)", "BIGINT", 'Sybase numeric type with no decimal places, using BIGINT']
  - ["", "", "numeric(2_,0)", "NUMERIC", 'Sybase Integer bigger than 32 bits, using NUMERIC - will probably need to change to BIGINT later']
  - ["staff", "password", "varchar(40)", "TEXT", 'Password in the staff table is varchar(40) but migrated value exceeds this length, using TEXT']

# Default values substitution
# column_name, source_column_data_type, source_default_value, target_default_value
default_values_substitution:
  - ["", "", "%getdate()%", "statement_timestamp()"]   ## condition with % to catch also "create default job_lastchange as getdate()", "(getdate())" and similar options
  - ["", "", "db_name()", "current_database()"]

# Substitutions for objects from other databases
# Informix, Sybase ASE and some other databases allow to use objects from other databases
# In PostgreSQL we must replace them with foreign tables linking to the original database
# "source_db:source_schema.source_object", "target_schema.target_object"
remote_objects_substitution:
  - ["remotedb:dbo.table1", "remote_db.table1"]

# Limitations for data migration
# "Table name (or pattern)", "condition for limiting data (without WHERE)", "column name (or pattern) - use condition when column is present in the table", "row limit"
#   - condition can contain placeholders {source_schema} and {source_table} to reference schema and table name for which condition will be applied
#   - placeholders will be replaced with proper values during migration
# row limit is optional, if specified, limitation will be applied only to tables with more than this number of rows
#   - if row limit is not specified, limitation will be applied to all tables matching the pattern and having the specified column
data_migration_limitation:
  - [".*", "date >= '2000-01-01'", "date", 1000000]
  - [".*", "movie_id in (select id from movies where date >= '2000-01-01')", "movie_id", 1000000]
  - ["movie_references", "referenced_id in (select id from movies where date >= '2000-01-01')", "referenced_id", 1000000]
  - [".*", "id >= (select max(id) from {source_schema}.{source_table})", "id", 1000000]


  - ["PK_RANGE", ".*", 1000000]

# Configurable partitioning for target tables
# partitioning:
#   - description: "partitioning for table1 by date"
#     table_name: "table1"
#     partition_by: "date1"  ## one or more columns, comma separated
#     partitioning_type: "range"  ## range, list, hash
#     date_range: month  ## year, month, week, day, hour
# date - by range, select unique values from the column from source table
# integer - by range or list
# hash - even data distribution

